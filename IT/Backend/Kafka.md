0# Kafka

## Сущности

**ZooKeeper** — инструмент-координатор, действует как общая служба конфигурации в системе. Работает как база для хранения метаданных о состоянии узлов кластера и расположении сообщений.

**Kafka Controller** — среди брокеров Zookeeper выбирает одного, который будет обеспечивать консистентность данных.

- **Топики** – это “папка” для сообщений. В топик складывается стрим данных, единая очередь из входящих сообщений.
- **Партиции** – это упорядоченная последовательность сообщений. Для ускорения чтения и записи топики делятся на партиции. Происходит параллелизация данных. Это конфигурируемый параметр, сообщения могут отправлять несколько продюсеров и принимать несколько консьюмеров.
- **Кластеры** – один или несколько хостов-брокеров, на которых размещены топики и соответствующие топикам разделы.
- **Брокеры** – это сервер, который принимает, хранит и обрабатывает потоки данных. Брокеры работают в кластере и объединяются вместе

Кластер → Брокеры (сервера) → Топики → Партиции → Сегменты.

Продьюсеры отправляют записи в кластеры, которые хранят записи и далее отправляют их консьюмерам. Каждая нода в кластере – это брокер, который хранит данные переданные продьюсером до тех пор, пока их не вычитает консьюмер.

![kafka-core](kafka-core.png)

### Продьсюеры

Продюсеры самостоятельно партицируют данные в топиках и сами определяют алгоритм партицирования: он может быть как банальный **_round-robin_** и **_hash-based_**, так и кастомный. 

Очерёдность сообщений гарантируется только для одной партиции.

### Records

Записи – это самый простой юнит в Kafka, event какой-то по факту. Содержит:

- Ключ
- Значение
- Заголовки
- Номер партиции
- Офсет
- Timestamp

### Offset

Офсет – это позиция (индекс) сообщения в определенном topic и partition, который указывает на то, какие сообщения были уже прочитаны consumer.

Если консьюмер падает в процессе получения данных, то, когда он запустится вновь и ему нужно будет вернутся к чтению этого сообщения, он воспользуется офсетом и продолжит с нужного места.

#### Где хранится оффсет?

В отдельном топике `consumer_offsets`. Ранее это было в ZK.

#### Committing offsets

Это процесс, посредством которого консьюмеры в Kafka сообщают брокеру о том, какие сообщения они уже успешно обработали.

В Kafka offsets могут быть автоматически зафиксированы (auto-commit) или управляемы вручную (manual commit).

#### Зачем нужно commit offsets?

- **Для обеспечения доставки "точно один раз" (exactly once delivery)**: Путем фиксации offsets потребители могут точно отслеживать, до какого момента в потоке данных они продвинулись. Это позволяет избежать повторной обработки уже прочитанных сообщений после перезапуска или сбоя потребителя.
- **Для гарантии упорядоченной обработки сообщений**: В системах, где порядок сообщений критичен, фиксация offsets обеспечивает возможность следовать этому порядку, даже если произошел сбой или необходимо перераспределить обработку между разными потребителями.

### Сегмент

Сегмент тоже удобно представить как обычный лог-файл: каждая следующая запись добавляется в конец файла и не меняет предыдущих записей. Фактически это очередь FIFO (First-In-First-Out) и Kafka реализует именно эту модель.

Семантически и физически сообщения внутри сегмента не могут быть удалены, они иммутабельны. Всё, что мы можем — указать, как долго Kafka-брокер будет хранить события через настройку политики устаревания данных или **_Retention Policy_**.

![kafka-log](kafka-log.png)

### Partition

Партиция — это упорядоченная и неизменяемая последовательность сообщений, которая хранит данные внутри топика.

Партиции распределены между брокерами — кластер Kafka делает это автоматически при создании топика.

![kafka-partitions-brokers](kafka-partitions-brokers.png)

#### Partition key

Нужен для того, чтобы распределять нагрузку по партициям. Хеш функция считает, например, на основе userID, в какую партицию попадет чел.

Либо же будет использован Round Robin.

#### Partition assignment stragegies

- RangeAssignor(1: 0-3, 2: 4-7,..)
- RoundRobinAssignor (1: 0, 2, 4…)
- StickyAssignor

### Consumer group

Консьюмер группа – это набор консьюмеров, объединенных одним ID, где вся группа слушает один топик и только один консьюмер вычитывает одно сообщение.

![kafka-2-partitions-2-consumers](kafka-2-partitions-2-consumers.png)

Если окажется, что нужно поставить третьего консьюмера, то надо ставить и третью партицию.

Или же, если нужно потреблять ивент другим consumer, то нужно создать отдельную consumer group.

![kafka-partitions-2-consumer-3](kafka-partitions-2-consumer-3.webp)

*Reference:* [What is a consumer group in Kafka? - Coding Harbour](https://codingharbour.com/apache-kafka/what-is-a-consumer-group-in-kafka/)

## Conditioning stages

## Репликация

Каждая партиция в Kafka имеет одну основную реплику, которая называется лидером. Именно лидер гарантирует упорядоченность сообщений в партиции и контролирует запись данных.

Каждая партиция также имеет дополнительные реплики, которые называются фолловерами. Реплики являются копиями данных из лидера. Они хранят данные в синхронизированном состоянии с лидером. Если лидер становится недоступным, одна из реплик может быстро перейти в роль лидера.

Лидер регулярно отправляет данные фолловерам, чтобы они могли обновлять свои копии данных и оставаться в синхронизированном состоянии.

![](kafka-liders-followers.png)
Клиенты, отправляющие запросы на запись данных, обращаются к лидеру. Лидер сохраняет упорядоченность и записывает данные, а затем реплицирует их на фолловеры.

>[!info] Фактор репликации отвечает за количество реплик.

### Балансировка и партицирование

![kafka-balancing](kafka-balancing.png)

Если фактор = 3, то будет 3 партиции на 3 брокерах, но партиции будут в разных ролях. Где-то в роли лидеров, а где-то фолловеров.

Программа-продюсер может указать ключ у сообщения и сама определяет номер партиции, например, разделяя вычисленный хеш на число партиций. Так она сохраняет сообщения одного идентификатора в одну и ту же партицию. Эта распространённая стратегия партицирования позволяет добиться строгой очерёдности событий при чтении за счёт сохранения сообщений в одну партицию, а не в разные.

Если программа-продюсер не указывает ключ, то стратегия партицирования по умолчанию называется **_round-robin_** — сообщения будут попадать в партиции по очереди. Эта стратегия хорошо работает в ряде бизнес-сценариев, где важна не очерёдность событий, а равномерное распределение сообщений между партициями. 

Также существуют **_List-based partitioning_**, **_Composite partitioning_**, **_Range-based partitioning_** и другие алгоритмы, каждый из которых подходит для своих задач. Вся логика реализации партицирования данных реализуется на стороне продюсера.

### Дизайн продьюсера

![kafka-producer-design](kafka-producer-design.png)

Payload упаковывается в структуру с указанием топика, партиции и ключа партицирования → сериализуется в подходящий формат — JSON / Protobuf → сообщению назначается партиция → группируется в пачки выбранных размеров и пересылается брокеру Kafka для сохранения.

### Ответственность ведущего брокера

Лидер (будь то брокер или партиция) отвечает за то, чтобы принимать сообщения. Он подтверждает запись данных продьюсеру  после того, как они записаны в лог.

Лидер отправляет данные всем репликам и ждет их подтверждения (в зависимости от настроек репликации).

Лидер партиции выбирается Kafka Controller из списка ISR. 

### Consumer Group Coordinator

Координатор отвечает за управление группами потребителей, отслеживание членов группы и распределение партиций среди них.

Он хранит оффсеты для каждой партиции, отслеживая, какие сообщения уже были обработаны потребителями.

При изменении состава группы потребителей (например, при добавлении или удалении потребителей) координатор перераспределяет партиции среди активных потребителей.

## Гарантии доставки Producer в Kafka

### acks

Параметр **acks** отвечает за вероятность потери сообщения.  Протокол Kafka предоставляет гарантии доставки всех трёх семантик: **_at-most once_**, **at-least once** и **_exactly-once_**.

- **acks=0**: Producer не ждет ответа от брокера, чтобы счесть отправку сообщения успешной. Принцип *Fire and forget*. Никаких гарантий доставки.
- **acks=1**. Producer ждет ответа от ведущего брокера, чтобы счесть отправку сообщений успешной. Но потеря все равно возможна, когда лидер отвалился, а реплика не имеет всей полноты данных. Такая аномалия возможна, если включен параметр *unclean.leader.election.enable*.
- **acks=all(-1)**. Producer ждет ответа от всех брокерах. Самый безопасный режим.

![kafka-gurantees](kafka-gurantees.png)

### retries

Количество предпринимаемых попыток отправить сообщение. По умолчанию продьюсер ждет 100 мс перед следующей попыткой.

### enable.indempotence

У продьюсера есть волшебный параметр, который защищает от дублирования сообщений. Каждому Producer присвается PID, а каждому сообщению возрастающий номер. Брокер отслеживает комбинацию PID + номер последовательности.

### Другие параметры

- request.timeout.ms – длительность ожидания ответа
- timeout.ms – длительность ожидания ответа репликами
- batch.size – объем в байтах для отправки. Как пакет заполнится данным объемом, то он отправляется.
- linger.ms - задержка, чтобы отправить сообщение с бОльшим пакетом
- max.in.flight.requests.per.connection - данный конфиг влияет на пропускную способность producer-a, позволяет отправлять N заапросов(батчей) на запись, которые еще не были потдверждены

### Producer с гарантией доставки точно один раз

1. **acks=all** – позволяет не думать о *unclean.leader.election.enable* и быть уверенным, что сообщение будет на брокере минимум один раз.
2. **retires > 0** (желательно >5)
3. **enable.indepotence=true**. Важно помнить , что гарантия распостраняется на сообщение, если оно при ретраях попадает в одну и ту же партицию, то есть нужен ключ партиционирования.
4. **request.timeout.ms** = 100ms
5. **max.in.flight.requests.per.connection** = 5


### Сохранение правильного порядка сообщений

Какой порядок считать правильным? – Время принятия сообщения.

## Гарантии доставки Consumer в Kafka

### enable.auto.commit

Будет ли потребитель коммитить смещения автоматически. Если нужно контроливровать, когда коммитится оффсет, то ставим false. Нужно для уменьшения степени дублирования и пропущенных данных.

Проблема авто коммита в том, что может в промежутке между двумя сообщениями произойти сбой консьюмера и тогда после перебалансировки консьюмер группы некоторые сообщения будут обработаны дважды.

По дефолту *true*.

### session.timeout.ms

Consumer отправляет hearbeat кластеру. Если в течение 10 секунд нет ответа, то происходит ребалансировка консьюмер группы.

### Идемпотетный consumer

Наилучший вариант, когда есть пара палей, образующие уникальную пару.
Либо messageID и uuid.New().
Про outbox….. processed_messages table.

### Producer-Consumer с гарантией ровно один раз

1. Producer с гарантией порядка сообщений и доставкой ровно один раз.
2. Идемпотетный консьюмер.

## Брокер

### replication.factor

Кол-во копий данных, который хранит кластер. Чем выше кэф, тем выше доступность и надежность.

### unclean.leader.election.enable

Если ведущая реплика становится недоступной, одна из согласованных реплик становится ведущей. Такой сценарий является “чистым”, то есть смена лидера произойдет без потери данных.

А что если чистой реплики нет?

1. ЖДем пока восстановится прежний лидер, будет потеря доступности
2. Разрешаем реплики с неполными данными стать лидером, будет потеря данных

По дефолту *true*.

### cleanup.policy (для сегментов)

Topic → Partitions → LogFile → Segment

Сегменты — это файлы на диске.

#### compact

Храним последнее сообщение для каждого ключа. То есть больше подходит для пары ключ-значения.

#### delete

Просто удаление сообщений по превышению объема или экспирации.

### Выборка лидера

В первую очередь выбирается самая актуальная реплика, которая в полной мере синхронизирована с предыдущим лидером (входит в список ISR - In-Sync Replicas).

#### Репликация партиций

1. **Синхронная и асинхронная репликация**:
    
    - **Синхронная репликация**: Реплики постоянно синхронизируются с лидером, копируя все новые сообщения. Лидер отправляет данные репликам и ждет подтверждения успешной записи.
    - **Асинхронная репликация**: Реплики могут немного отставать от лидера, не гарантируя немедленную запись всех данных.
      
2. **In-Sync Replicas (ISR)**:
    
    - **Список актуальных реплик**: ISR содержит реплики, которые находятся в полной синхронизации с лидером. Лидер ждет подтверждения записи от всех реплик из ISR перед подтверждением клиенту.
    - **Механизм исключения**: Если реплика отстает от лидера на значительное время или по количеству сообщений, она исключается из ISR.

### Kafka is PULL based

Major points that were in favor of pull are:

1. Pull is better in dealing with diversified consumers (without a broker determining the data transfer rate for all);
2. Consumers can more effectively control the rate of their individual consumption;
3. Easier and more optimal batch processing implementation.

## Виды ошибок консьюмеров

- Network/Timeout
- DataInvalid/UserInput

## Флоу сообщения

#### 1. Продюсер отправляет сообщение

- Сообщение содержит ключ, значение и метаданные (включая информацию о топике и партиции).

#### 2. Получение сообщения лидером партиции

- Лидер партиции получает сообщение от продюсера.
- Лидер записывает сообщение в лог-файл соответствующей партиции.
- Сообщение получает уникальное смещение (offset) в логе.

Сообщение сохраняется в сегменте лог-файла на диске лидера.

#### 3. Репликация сообщения на фолловеры

- Лидер отправляет сообщение всем репликам (фолловерам) партиции.
- Фолловеры принимают сообщение и записывают его в свои лог-файлы.

Сообщение копируется в лог-файлы фолловеров на других брокерах.

**Важно:** Процесс считается завершенным, когда все фолловеры из списка ISR (In-Sync Replicas) подтвердят получение сообщения.

#### 4. Координатор группы потребителей отслеживает оффсеты

- Координатор отслеживает, какие сообщения были прочитаны потребителями.
- Гарантирует, что каждое сообщение будет обработано хотя бы одним потребителем в группе.

#### 5. Консьюмер запрашивает данные

- Консьюмер из группы запрашивает данные у лидера партиции.
- Лидер отправляет потребителю сообщения, начиная с последнего зафиксированного оффсета.

Сообщения передаются из лога лидера в потребительское приложение.


## Метрики

### Consumer

- Кол-во сообщений, которое попало в retry topic в разрезе топика и партиции
- Кол-во сообщений, которое попало в DLQ topic
- Общее кол-во retry
- Лаг на топик в разрезе партиици и топика
- Кол-во успешных коммитов в разрезе топика и партиции
- Квантили скорости обработки одного сообщений в разрезе топика и партиции

### Producer

1. Rate записи сообщений с разрезом по топику и статусу
2. Записи в outbox
3. Метрика старта крон джобы аутбокса

## HWM

High Water Mark – оффсет последнего сообщения, которое было реплицировано между всеми репликами партиции. Находится внутри лидерующей реплики партиции

### Как консьюмер узнает о HWM?

Через метадату, которую возвращает брокер, когда консьюмер впервые подключается к топику. Метадата включает в себя:

- текущего лидера
- реплики
- текущий HWM для каждой реплики

## LEO

Log End Offset – оффсет, который был записан в лог партиции. Устанавливается каждой репликой, то есть может различаться в разных репликах.

По-хорошему, они $LEO == HWM$, но на практике, если один брокер страдает, то разница между ними будет говорить о проблеме. 

## Преимущества Kafka

#### Горизонтальное масштабирование

Множество объединенных серверов гарантируют высокую доступность данных — выход из строя одного из узлов не нарушает целостность. Кластер состоит из обычных машин, а не суперкомпьютеров, их можно менять и дополнять. Система автоматически перебалансируется. 

Чтобы события не потерялись, существуют механизмы репликации. Данные записываются на несколько машин, если что-то случается с сервером, он переключается на резервный. Кластер в режиме реального времени определяет, где находятся данные, и продолжает их использовать.


